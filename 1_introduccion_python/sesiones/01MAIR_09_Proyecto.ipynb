{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Titanic data science project\n",
    "- End-to-end en data science (Clasificacion)\n",
    "- Fuente: https://www.kaggle.com/jeffd23/scikit-learn-ml-from-start-to-finish#\n",
    "- Solucion del Titanic Survival competition en Kaggle: https://www.kaggle.com/c/titanic"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# importar paquetes necesarios\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import os\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# cargar el dataset https://www.kaggle.com/c/titanic/data\n",
    "\n",
    "ruta_train = os.path.join(\"res\", \"titanic\", \"train.csv\")\n",
    "ruta_test = os.path.join(\"res\", \"titanic\", \"test.csv\")\n",
    "\n",
    "train_data = pd.read_csv(ruta_train)\n",
    "test_data = pd.read_csv(ruta_test)\n",
    "display(train_data.sample(3))\n",
    "display(test_data.sample(3))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(train_data.shape)\n",
    "print(test_data.shape)\n",
    "display(train_data.describe(include = 'all'))\n",
    "display(test_data.describe(include = 'all'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# visualizacion\n",
    "sns.barplot(x='Embarked',y='Survived',hue='Sex',data=train_data)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.pointplot(x='Pclass',y='Survived',hue='Sex',data=train_data,palette={'male':'red', 'female':'blue'})\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# limpieza de datos\n",
    "# categorizacion de Age (edad, en categorias)\n",
    "so = list(train_data.Age.unique())\n",
    "so.sort()\n",
    "print(so)\n",
    "# ¿Y el test set?\n",
    "train_data.hist(column='Age')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creación de categorías por rangos, discretizar\n",
    "def cat_age(data):\n",
    "    data['Age'] = data['Age'].fillna(-0.5)\n",
    "    bins = (-1,0,12,18,30,60,120)\n",
    "    cat_names = ['Unknown','Child','Teenager','Young adult','Adult','Elderly']\n",
    "    categories = pd.cut(data['Age'],bins,labels=cat_names)\n",
    "    data['Age'] = categories\n",
    "    return data\n",
    "\n",
    "train_data = cat_age(train_data)\n",
    "test_data = cat_age(test_data)\n",
    "\n",
    "train_data['Age'].sample(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Conocer la distribución de las cabinas con un count plot\n",
    "sns.countplot(x='Cabin', data=train_data)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# extraccion de la letra de la cabina (no numero)\n",
    "def extract_cabin(data):\n",
    "    data['Cabin'] = data['Cabin'].fillna('N')\n",
    "    data['Cabin'] = data['Cabin'].apply(lambda x : x[0])\n",
    "    return data\n",
    "\n",
    "train_data = extract_cabin(train_data)\n",
    "test_data = extract_cabin(test_data)\n",
    "\n",
    "train_data.Cabin.sample(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# categorizacion de Fare (precio del ticket, en cuartiles)\n",
    "def cat_fare(data):\n",
    "    data['Fare'] = data['Fare'].fillna(-0.5)\n",
    "    cat_names = ['Unknown','1st','2nd','3rd','4rd']\n",
    "    data['Fare'] = pd.qcut(data['Fare'],5,labels=cat_names)\n",
    "    return data\n",
    "\n",
    "train_data = cat_fare(train_data)\n",
    "test_data = cat_fare(test_data)\n",
    "\n",
    "train_data.Fare.sample(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# extraer info del nombre (Mr, Ms...)\n",
    "def extract_title(data):\n",
    "    data['Title'] = data['Name'].apply(lambda x : x.split(' ')[1])\n",
    "    return data\n",
    "\n",
    "train_data = extract_title(train_data)\n",
    "test_data = extract_title(test_data)\n",
    "\n",
    "train_data.Title.sample(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Eliminacion de datos poco informativos (Name, Ticket)\n",
    "def drop_columns(data,columns):\n",
    "    return data.drop(columns,axis=1)\n",
    "\n",
    "columns_to_drop = ['Name','Ticket','Embarked']\n",
    "train_data = drop_columns(train_data,columns_to_drop)\n",
    "test_data = drop_columns(test_data,columns_to_drop)\n",
    "\n",
    "train_data.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# representar los datos categoricos\n",
    "# Edad vs supervivencia\n",
    "sns.barplot(x='Age',y='Survived',hue='Sex',data=train_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Precio del ticket vs supervivencia\n",
    "sns.barplot(x='Fare',y='Pclass',hue='Sex',data=train_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Preparar para machine learning\n",
    "# transformar datos en etiquetas numericas\n",
    "from sklearn import preprocessing\n",
    "def encode_features(df_train, df_test):\n",
    "    features = ['Fare', 'Cabin', 'Age', 'Sex', 'Title']\n",
    "    df_combined = pd.concat([df_train[features], df_test[features]])\n",
    "    \n",
    "    for feature in features:\n",
    "        le = preprocessing.LabelEncoder()\n",
    "        le = le.fit(df_combined[feature])\n",
    "        df_train[feature] = le.transform(df_train[feature])\n",
    "        df_test[feature] = le.transform(df_test[feature])\n",
    "        \n",
    "    return df_train, df_test\n",
    "    \n",
    "train_data, test_data = encode_features(train_data, test_data)\n",
    "train_data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# dividir train/validating sets\n",
    "X = train_data.drop(['Survived'],axis=1)\n",
    "Y = train_data['Survived']\n",
    "\n",
    "validation_size=0.15 # qué porcentaje del dataset nos servirá para validar\n",
    "seed = np.random.randint(1000)\n",
    "X_train, X_validation, Y_train, Y_validation = train_test_split(X, Y, test_size=validation_size, random_state=seed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Decision tree ML\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "model = DecisionTreeClassifier()\n",
    "model.fit(X_train,Y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Evaluacion\n",
    "# test accuracy on validation set\n",
    "from sklearn.metrics import classification_report\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "prediction = model.predict(X_validation)\n",
    "print('Accuracy: {}'.format(accuracy_score(prediction,Y_validation)))\n",
    "print(confusion_matrix(Y_validation,prediction))\n",
    "print(classification_report(Y_validation,prediction))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Se eliminan columnas que pueden tener poca/nula relevancia en el entrenamiento\n",
    "X = train_data.drop(['Survived', 'PassengerId'],axis=1)\n",
    "Y = train_data['Survived']\n",
    "\n",
    "validation_size=0.15\n",
    "X_train, X_validation, Y_train, Y_validation = train_test_split(X, Y, test_size=validation_size, random_state=seed)\n",
    "\n",
    "model = DecisionTreeClassifier()\n",
    "model.fit(X_train,Y_train)\n",
    "\n",
    "prediction = model.predict(X_validation)\n",
    "print('Accuracy: {}'.format(accuracy_score(prediction,Y_validation)))\n",
    "print(confusion_matrix(Y_validation,prediction))\n",
    "print(classification_report(Y_validation,prediction))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = train_data.drop(['Survived', 'PassengerId'],axis=1)\n",
    "Y = train_data['Survived']\n",
    "\n",
    "ls = []\n",
    "for i in range(5):\n",
    "    validation_size=0.15\n",
    "    X_train, X_validation, Y_train, Y_validation = train_test_split(X, Y, test_size=validation_size, random_state=seed)\n",
    "\n",
    "    model = DecisionTreeClassifier()\n",
    "    model.fit(X_train,Y_train)\n",
    "\n",
    "    prediction = model.predict(X_validation)\n",
    "    ls.append(accuracy_score(prediction,Y_validation))\n",
    "    \n",
    "print(ls)\n",
    "sum(ls)/len(ls)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Iris PCA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "df_iris = sns.load_dataset('iris')\n",
    "df_iris.sample(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.pairplot(data=df_iris, hue='species')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "display(df_iris.corr())\n",
    "sns.heatmap(df_iris.corr(), square=True, annot=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# calc autovectores/autovalores\n",
    "from sklearn.decomposition import PCA  #Analisis de componentes principales\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "#normalizacion de datos, eliminar variables no predictoras\n",
    "df_pca = df_iris.drop('species', axis=1)\n",
    "display(df_pca.head())\n",
    "\n",
    "x_scaled = StandardScaler()\n",
    "x_scaled.fit(df_pca)\n",
    "scaled = x_scaled.transform(df_pca)\n",
    "print(scaled)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#analisis de componentes principales PCA\n",
    "#reducir dimensiones y generar autovectores que reunan todas las caracteristicas (acumulan varianza) de 4 a 3\n",
    "n_components = 3\n",
    "pca = PCA(n_components = n_components)\n",
    "pca.fit(scaled)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_pca = pca.transform(scaled)\n",
    "x_pca.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#varianza acumulada en los autovectores (explica caracteristicas que reunen el cjto de valores)\n",
    "plt.plot(np.cumsum(pca.explained_variance_ratio_))\n",
    "plt.show()\n",
    "#comp1 0.72 varianza acumulada\n",
    "#comp2 0.94 varianza acumulada\n",
    "#comp3 0.98 varianza acumulada\n",
    "columns = [i for i in pca.explained_variance_ratio_]\n",
    "columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "columns = [f'PC{i+1}' for i, v in enumerate(pca.explained_variance_ratio_)]\n",
    "columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.DataFrame(x_pca, columns=columns)  #autovectores \n",
    "display(df)\n",
    "df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#grafico de dispersion de 2 primeras componentes\n",
    "plt.scatter(df.PC1, df.PC2)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.scatterplot(data=df, x='PC1', y='PC2', hue='species')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#no hay species en el dataframe\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#hay que hacer join/merge para traer las species\n",
    "df_joined = df.join(df_iris['species'], how='inner')\n",
    "display(df_joined.sample(5))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#las tres species se pueden separar bien (PC1 y PC2 absorben la mayor parte de la varianza)\n",
    "sns.scatterplot(data=df_joined, x='PC1', y='PC2', hue='species')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pca.explained_variance_ratio_"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
