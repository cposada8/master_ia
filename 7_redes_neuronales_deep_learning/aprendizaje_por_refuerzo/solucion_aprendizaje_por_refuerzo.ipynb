{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.7.1"
    },
    "colab": {
      "name": "Introducción al aprendizaje por refuerzo - Ejercicio de evaluación.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "toc_visible": true
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bAkM4_YMDNRP"
      },
      "source": [
        "# Trabajo Final: Aprendizaje por Refuerzo\n",
        "\n",
        "__Carlos Esteban Posada Mejia__\n",
        "\n",
        "__crls.esteban@hotmail.com__\n",
        "\n",
        "Este trabajo se divide en 2 partes:\n",
        "1. Parte teórica\n",
        "2. Parte práctica"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TWKpWaGfAKVa"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ut_ECridJjNw"
      },
      "source": [
        "## Teoría"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uw9Usi7yJjN2"
      },
      "source": [
        "- Esta parte será el 40% de la nota final del bloque de Aprendizaje por Refuerzo."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Qt4fH00AJjN2"
      },
      "source": [
        "### Pregunta 1\n",
        "Define brevemente qué es el aprendizaje por refuerzo. ¿Qué diferencias hay entre aprendizaje supervisado, no supervisado y por refuerzo?"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "H9QLyIXZDsht"
      },
      "source": [
        "#### Respuesta:\n",
        "\n",
        "El aprendizaje por refuerzo es una técnica de aprendizaje de máquinas en el cuál un agente \"aprende\" a tomar decisiones de acuerdo a su entorno. \n",
        "\n",
        "Este aprendizaje se da por medio de la interacción del agente con su medio y de experimentar los efectos que tienen sus acciones. \n",
        "\n",
        "La forma por la cuál \"entiende\" las consecuencias de sus acciones es por medio de una recompensa.\n",
        "\n",
        "Cabe señalar que existe un paradigma en el cual no sólo un agente aprende sino que se trata de ambientes multiagente.\n",
        "\n",
        "\n",
        "__Diferencias entre aprendizaje supervisado, no supervisado y por refuerzo:__\n",
        "El aprendizaje supervisado se caracteriza por tener una variable objetivo, la cuál debe ser estimada basándose en las variables independientes. \n",
        "\n",
        "El aprendizaje NO supervisado se caracteriza justamente por no contar con esta variable objetivo, y sus tareas son las de obtener información y patrones subyacentes en los datos. \n",
        "\n",
        "El aprendizaje por refuerzo no busca estimar una variable objetivo, pero sí maximizar una recomensa. Por lo cuál no es completamente falto de un objetivo (como el aprendizaje no supervisado) pero tampoco estima un target (como el aprendizaje supervisado). \n",
        "Trata más de aprender por medio de ensayo-error, de las consecuencias de sus acciones, buscando maximizar la recompensa."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4_ET8uNXDo-H"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3nmPDM7wJjN3"
      },
      "source": [
        ""
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "p8MMwx9SJjN3"
      },
      "source": [
        "### Pregunta 2\n",
        "Define con tus palabras los conceptos de Entorno, Agente, Recompensa, Estado y Observación."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "erJtiR89EZex"
      },
      "source": [
        "#### Respuesta:\n",
        "Un __entorno__ es un medio en el cuál un agente interactúa con otros elementos del mismo (entorno). El entorno afecta y es afectado por las acciones del agente y a su vez provee esta información al agente para que pueda aprender sobre las consecuencias de sus acciones.\n",
        "\n",
        "Un __agente__ es un ente inmerso en un entorno. Con cierta capacidad para tomar decisiones. Estas decisiones se convierten en acciones y las acciones modifican el entorno, ocasionando a su vez que el entorno afecte al agente. \n",
        "\n",
        "La __recompensa__ es un mecanismo que sirve para calificar las acciones de un agente. Si una acción es adecuada, se derivará de ellos una recompensa buena, mientras que si la acción no fue la mejor, se puede dar una menor recompensa o un castigo.\n",
        "\n",
        "Yo definiría el __estado__ como una configuración del entorno. Podríamos pensar que el entorno está compuesto por diversas \"variables\", por ejemplo, en el juego de _space invaders_ el entorno podría contener:\n",
        "* número de aliens\n",
        "* posición de cada alien\n",
        "* posición de la nave\n",
        "entre otras.\n",
        "El estado vendría siendo una configuración particular de todas las variables del entorno.\n",
        "\n",
        "Una __observación__ la definiría como la \"foto\" del entorno. Es una forma de conocer el estado del entorno (y los agentes en él) en un momento determinado."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lTMkqZnOEX1a"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "C8S39eHVJjN3"
      },
      "source": [
        ""
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8lCsBZM4JjN3"
      },
      "source": [
        "### Pregunta 3\n",
        "Dependiendo del algoritmo de aprendizaje por refuerzo que se use, ¿qué clasificaciones podemos encontrar? Coméntalas brevemente."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "O6ztDVYZJ_sb"
      },
      "source": [
        "#### Respuesta:\n",
        "Podemos encontrar 2 tipos de algoritmos: Basados en modelo y basados en estrategia\n",
        "\n",
        "* Basados en modelo: En esta categoría podemos tener 2 tipos de enfoques:\n",
        "    * Model Free: no se cuenta con un modelo para predecir los estados futuros del entorno. En esta categoría el agente es dependiente sólo del estado actual del entorno. La mayoría de aplicaciones actuales se encuentran en esta categoría (por ejemplo space invaders), porque no podemos estimar dónde ni cuándo van a aparecer nuevos aliens, dado que esto responde a una generación aleatoria.\n",
        "\n",
        "    * Model Based: se conoce un modelo para estimar los futuros estados del entorno. Los problemas en esta clasificación se caracterizan por tener que implementar heurísticas y técnicas de optimización para realizar búsquedas óptimas y planes que maximicen la recompensa futuro. Ejemplos de problemas en esta categoría son los juegos como Go, Ajedrez, donde tienes un modelo para ver los posibles estados dada tu acción.\n",
        "\n",
        "* Basados en estrategia: esta clasificación se refiere a las acciones que se toman como respuesta a la observación del estado del entorno. Dependiendo de la etapa de entrenamiento en la que estemos y de la experiencia recolectada, la estrategia habla de qué decisión tomar. Tenemos también 2 clasificaciones:\n",
        "    * on policy: acá se encuentran las técnicas de policy gradients. Hace uso de la experiencia para determinar \"la mejor\" decisión teniendo en cuenta el estado. \n",
        "\n",
        "    * off policy: Esta estrategia es la contraria de on-policy. Pues si bien observa la experiencia, no necesariamente actúa de acuerdo a lo vivido, generando acciones con cierto componente aleatorio que le ayuda a descubrir nuevos caminos. Se caracteriza por largos tiempos de entrenamiento."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qfdDw2swJ3kx"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "foauTts3JjN4"
      },
      "source": [
        ""
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "soeg28-tJjN4"
      },
      "source": [
        "### Pregunta 4\n",
        "Lista tres diferencias entre los algoritmos de DQN y Policy Gradient"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hvBS0Rjiy6OJ"
      },
      "source": [
        "#### Respuesta:\n",
        "* La principal diferencia que mencionaría es que DQN trata de estimar la recompensa de modo que encuentre una configuración de acciones o estrategia, mientras que policy gradient estima la acción en sí. Esto suena similar, pero para efectos prácticos es distinto.\n",
        "\n",
        "* Las técnicas basadas en Q-learning como DQN usan como función a optimizar la _ecuación de bellman_ mientras que en policy gradient se suele optimizar la función de _cross-entropy_\n",
        "\n",
        "* Si bien ambas técnicas vienen en la rama de \"model free\", DQN pertenece a las técnicas de _Q-learning_ mientras que policy gradiente hace parte de las técnicas de _Policy Optimization_"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0C8UfOY3y4o3"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1fAzaO6dOtwh"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "55fbY2xYJjN4"
      },
      "source": [
        ""
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ThrV4Tm8JjN4"
      },
      "source": [
        "## Práctica"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5lj0ENdCJjN4"
      },
      "source": [
        "- Esta parte será el 60% de la nota final del bloque de Aprendizaje por Refuerzo."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rTVpo6uLJjN5"
      },
      "source": [
        "Algunas consideraciones a tener en cuenta:\n",
        "\n",
        "- El entorno sobre el que trabajaremos será _SpaceInvaders-v0_ y el algoritmo que usaremos será _DQN_.\n",
        "\n",
        "- Para nuestro ejercicio, una solución óptima será alcanzar una media de recompensa por encima de 16 puntos. Para medir si hemos conseguido llegar a la solución óptima, la media de la recompensa se calculará a partir del código de test en la última celda del notebook."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jq2OKHgaJjN5"
      },
      "source": [
        "Este bloque práctico consta de tres partes:\n",
        "\n",
        "   1) Implementar la red neuronal que se usará en la solución\n",
        "    \n",
        "   2) Seleccionar los hiperparámetros adecuados para las distintas piezas de la solución DQN\n",
        "    \n",
        "   3) Justificar la respuesta en relación a los resultados obtenidos"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OpRa0LccJjN5"
      },
      "source": [
        "IMPORTANTE:\n",
        "\n",
        "- Si no se consigue una puntuación óptima, responder sobre la mejor puntuación obtenida.\n",
        "\n",
        "- Para entrenamientos largos, recordad que podéis usar checkpoints de vuestros modelos para retomar los entrenamientos. En este caso, recordad cambiar los parámetros adecuadamente (sobre todo los relacionados con el proceso de exploración).\n",
        "\n",
        "- Si usáis Google Colab, recordad usar las versiones de Tensorflow==1.13.1, Keras==2.2.4 y keras-rl==0.4.2"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IkRx7ipY_O-C"
      },
      "source": [
        "### Instalación y carga de las bibliotecas"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "rSLnBKLUJvH9",
        "outputId": "f094c399-fb97-4855-a9ac-57fda17d27a9"
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive/')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Mounted at /content/drive/\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wpsZC6GUJyX1",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "2eebcd0c-5134-44b2-b863-da6614b417b6"
      },
      "source": [
        "# montamos el BASE_FOLDER para interactuar con archivos del drive\n",
        "import os\n",
        "BASE_FOLDER = \"./drive/MyDrive/Reinforcement_learning/\"\n",
        "os.listdir(BASE_FOLDER)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['Introducción al aprendizaje por refuerzo - Ejercicio de evaluación.ipynb']"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 2
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0OKCs8tFJzdc"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "SwKiYl0KJjN6",
        "outputId": "dab4178e-cca6-4d57-9d9e-69fea9b41512"
      },
      "source": [
        "# Instalación de bibliotecas\n",
        "!pip install keras-rl\n",
        "!pip freeze | grep Keras\n",
        "!pip freeze | grep keras-rl\n",
        "!pip freeze | grep tensorflow\n",
        "!pip install h5py\n",
        "!pip install Pillow\n",
        "!pip install gym[atari]\n",
        "!pip install keras-rl==0.4.2\n",
        "!pip install tensorflow==1.13.1\n",
        "!pip install Keras==2.2.4\n",
        "!pip install jupyter\n",
        "!pip install torch"
      ],
      "execution_count": 25,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Collecting keras-rl\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/ab/87/4b57eff8e4bd834cea0a75cd6c58198c9e42be29b600db9c14fafa72ec07/keras-rl-0.4.2.tar.gz (40kB)\n",
            "\r\u001b[K     |████████                        | 10kB 21.6MB/s eta 0:00:01\r\u001b[K     |████████████████▏               | 20kB 29.2MB/s eta 0:00:01\r\u001b[K     |████████████████████████▎       | 30kB 22.2MB/s eta 0:00:01\r\u001b[K     |████████████████████████████████| 40kB 5.8MB/s \n",
            "\u001b[?25hRequirement already satisfied: keras>=2.0.7 in /usr/local/lib/python3.7/dist-packages (from keras-rl) (2.4.3)\n",
            "Requirement already satisfied: pyyaml in /usr/local/lib/python3.7/dist-packages (from keras>=2.0.7->keras-rl) (3.13)\n",
            "Requirement already satisfied: h5py in /usr/local/lib/python3.7/dist-packages (from keras>=2.0.7->keras-rl) (2.10.0)\n",
            "Requirement already satisfied: scipy>=0.14 in /usr/local/lib/python3.7/dist-packages (from keras>=2.0.7->keras-rl) (1.4.1)\n",
            "Requirement already satisfied: numpy>=1.9.1 in /usr/local/lib/python3.7/dist-packages (from keras>=2.0.7->keras-rl) (1.19.5)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.7/dist-packages (from h5py->keras>=2.0.7->keras-rl) (1.15.0)\n",
            "Building wheels for collected packages: keras-rl\n",
            "  Building wheel for keras-rl (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for keras-rl: filename=keras_rl-0.4.2-cp37-none-any.whl size=48380 sha256=a3c190117700abf035c433e87bbe995119530026131eaf01763c54717acb75cf\n",
            "  Stored in directory: /root/.cache/pip/wheels/7d/4d/84/9254c9f2e8f51865cb0dac8e79da85330c735551d31f73c894\n",
            "Successfully built keras-rl\n",
            "Installing collected packages: keras-rl\n",
            "Successfully installed keras-rl-0.4.2\n",
            "Keras==2.4.3\n",
            "Keras-Preprocessing==1.1.2\n",
            "keras-rl==0.4.2\n",
            "tensorflow==2.4.1\n",
            "tensorflow-datasets==4.0.1\n",
            "tensorflow-estimator==2.4.0\n",
            "tensorflow-gcs-config==2.4.0\n",
            "tensorflow-hub==0.12.0\n",
            "tensorflow-metadata==0.29.0\n",
            "tensorflow-probability==0.12.1\n",
            "Requirement already satisfied: h5py in /usr/local/lib/python3.7/dist-packages (2.10.0)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.7/dist-packages (from h5py) (1.15.0)\n",
            "Requirement already satisfied: numpy>=1.7 in /usr/local/lib/python3.7/dist-packages (from h5py) (1.19.5)\n",
            "Requirement already satisfied: Pillow in /usr/local/lib/python3.7/dist-packages (7.1.2)\n",
            "Requirement already satisfied: gym[atari] in /usr/local/lib/python3.7/dist-packages (0.17.3)\n",
            "Requirement already satisfied: scipy in /usr/local/lib/python3.7/dist-packages (from gym[atari]) (1.4.1)\n",
            "Requirement already satisfied: cloudpickle<1.7.0,>=1.2.0 in /usr/local/lib/python3.7/dist-packages (from gym[atari]) (1.3.0)\n",
            "Requirement already satisfied: pyglet<=1.5.0,>=1.4.0 in /usr/local/lib/python3.7/dist-packages (from gym[atari]) (1.5.0)\n",
            "Requirement already satisfied: numpy>=1.10.4 in /usr/local/lib/python3.7/dist-packages (from gym[atari]) (1.19.5)\n",
            "Requirement already satisfied: atari-py~=0.2.0; extra == \"atari\" in /usr/local/lib/python3.7/dist-packages (from gym[atari]) (0.2.6)\n",
            "Requirement already satisfied: Pillow; extra == \"atari\" in /usr/local/lib/python3.7/dist-packages (from gym[atari]) (7.1.2)\n",
            "Requirement already satisfied: opencv-python; extra == \"atari\" in /usr/local/lib/python3.7/dist-packages (from gym[atari]) (4.1.2.30)\n",
            "Requirement already satisfied: future in /usr/local/lib/python3.7/dist-packages (from pyglet<=1.5.0,>=1.4.0->gym[atari]) (0.16.0)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.7/dist-packages (from atari-py~=0.2.0; extra == \"atari\"->gym[atari]) (1.15.0)\n",
            "Requirement already satisfied: keras-rl==0.4.2 in /usr/local/lib/python3.7/dist-packages (0.4.2)\n",
            "Requirement already satisfied: keras>=2.0.7 in /usr/local/lib/python3.7/dist-packages (from keras-rl==0.4.2) (2.4.3)\n",
            "Requirement already satisfied: pyyaml in /usr/local/lib/python3.7/dist-packages (from keras>=2.0.7->keras-rl==0.4.2) (3.13)\n",
            "Requirement already satisfied: scipy>=0.14 in /usr/local/lib/python3.7/dist-packages (from keras>=2.0.7->keras-rl==0.4.2) (1.4.1)\n",
            "Requirement already satisfied: numpy>=1.9.1 in /usr/local/lib/python3.7/dist-packages (from keras>=2.0.7->keras-rl==0.4.2) (1.19.5)\n",
            "Requirement already satisfied: h5py in /usr/local/lib/python3.7/dist-packages (from keras>=2.0.7->keras-rl==0.4.2) (2.10.0)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.7/dist-packages (from h5py->keras>=2.0.7->keras-rl==0.4.2) (1.15.0)\n",
            "Collecting tensorflow==1.13.1\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/d4/29/6b4f1e02417c3a1ccc85380f093556ffd0b35dc354078074c5195c8447f2/tensorflow-1.13.1-cp37-cp37m-manylinux1_x86_64.whl (92.6MB)\n",
            "\u001b[K     |████████████████████████████████| 92.6MB 47kB/s \n",
            "\u001b[?25hCollecting tensorboard<1.14.0,>=1.13.0\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/0f/39/bdd75b08a6fba41f098b6cb091b9e8c7a80e1b4d679a581a0ccd17b10373/tensorboard-1.13.1-py3-none-any.whl (3.2MB)\n",
            "\u001b[K     |████████████████████████████████| 3.2MB 46.2MB/s \n",
            "\u001b[?25hRequirement already satisfied: wheel>=0.26 in /usr/local/lib/python3.7/dist-packages (from tensorflow==1.13.1) (0.36.2)\n",
            "Requirement already satisfied: absl-py>=0.1.6 in /usr/local/lib/python3.7/dist-packages (from tensorflow==1.13.1) (0.12.0)\n",
            "Requirement already satisfied: termcolor>=1.1.0 in /usr/local/lib/python3.7/dist-packages (from tensorflow==1.13.1) (1.1.0)\n",
            "Requirement already satisfied: grpcio>=1.8.6 in /usr/local/lib/python3.7/dist-packages (from tensorflow==1.13.1) (1.32.0)\n",
            "Requirement already satisfied: numpy>=1.13.3 in /usr/local/lib/python3.7/dist-packages (from tensorflow==1.13.1) (1.19.5)\n",
            "Requirement already satisfied: protobuf>=3.6.1 in /usr/local/lib/python3.7/dist-packages (from tensorflow==1.13.1) (3.12.4)\n",
            "Requirement already satisfied: keras-preprocessing>=1.0.5 in /usr/local/lib/python3.7/dist-packages (from tensorflow==1.13.1) (1.1.2)\n",
            "Requirement already satisfied: six>=1.10.0 in /usr/local/lib/python3.7/dist-packages (from tensorflow==1.13.1) (1.15.0)\n",
            "Collecting tensorflow-estimator<1.14.0rc0,>=1.13.0\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/bb/48/13f49fc3fa0fdf916aa1419013bb8f2ad09674c275b4046d5ee669a46873/tensorflow_estimator-1.13.0-py2.py3-none-any.whl (367kB)\n",
            "\u001b[K     |████████████████████████████████| 368kB 54.2MB/s \n",
            "\u001b[?25hRequirement already satisfied: gast>=0.2.0 in /usr/local/lib/python3.7/dist-packages (from tensorflow==1.13.1) (0.3.3)\n",
            "Collecting keras-applications>=1.0.6\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/71/e3/19762fdfc62877ae9102edf6342d71b28fbfd9dea3d2f96a882ce099b03f/Keras_Applications-1.0.8-py3-none-any.whl (50kB)\n",
            "\u001b[K     |████████████████████████████████| 51kB 8.8MB/s \n",
            "\u001b[?25hRequirement already satisfied: astor>=0.6.0 in /usr/local/lib/python3.7/dist-packages (from tensorflow==1.13.1) (0.8.1)\n",
            "Requirement already satisfied: werkzeug>=0.11.15 in /usr/local/lib/python3.7/dist-packages (from tensorboard<1.14.0,>=1.13.0->tensorflow==1.13.1) (1.0.1)\n",
            "Requirement already satisfied: markdown>=2.6.8 in /usr/local/lib/python3.7/dist-packages (from tensorboard<1.14.0,>=1.13.0->tensorflow==1.13.1) (3.3.4)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.7/dist-packages (from protobuf>=3.6.1->tensorflow==1.13.1) (56.0.0)\n",
            "Collecting mock>=2.0.0\n",
            "  Downloading https://files.pythonhosted.org/packages/5c/03/b7e605db4a57c0f6fba744b11ef3ddf4ddebcada35022927a2b5fc623fdf/mock-4.0.3-py3-none-any.whl\n",
            "Requirement already satisfied: h5py in /usr/local/lib/python3.7/dist-packages (from keras-applications>=1.0.6->tensorflow==1.13.1) (2.10.0)\n",
            "Requirement already satisfied: importlib-metadata; python_version < \"3.8\" in /usr/local/lib/python3.7/dist-packages (from markdown>=2.6.8->tensorboard<1.14.0,>=1.13.0->tensorflow==1.13.1) (3.10.1)\n",
            "Requirement already satisfied: typing-extensions>=3.6.4; python_version < \"3.8\" in /usr/local/lib/python3.7/dist-packages (from importlib-metadata; python_version < \"3.8\"->markdown>=2.6.8->tensorboard<1.14.0,>=1.13.0->tensorflow==1.13.1) (3.7.4.3)\n",
            "Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.7/dist-packages (from importlib-metadata; python_version < \"3.8\"->markdown>=2.6.8->tensorboard<1.14.0,>=1.13.0->tensorflow==1.13.1) (3.4.1)\n",
            "Installing collected packages: tensorboard, mock, tensorflow-estimator, keras-applications, tensorflow\n",
            "  Found existing installation: tensorboard 2.4.1\n",
            "    Uninstalling tensorboard-2.4.1:\n",
            "      Successfully uninstalled tensorboard-2.4.1\n",
            "  Found existing installation: tensorflow-estimator 2.4.0\n",
            "    Uninstalling tensorflow-estimator-2.4.0:\n",
            "      Successfully uninstalled tensorflow-estimator-2.4.0\n",
            "  Found existing installation: tensorflow 2.4.1\n",
            "    Uninstalling tensorflow-2.4.1:\n",
            "      Successfully uninstalled tensorflow-2.4.1\n",
            "Successfully installed keras-applications-1.0.8 mock-4.0.3 tensorboard-1.13.1 tensorflow-1.13.1 tensorflow-estimator-1.13.0\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.colab-display-data+json": {
              "pip_warning": {
                "packages": [
                  "tensorboard",
                  "tensorflow"
                ]
              }
            }
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "stream",
          "text": [
            "Collecting Keras==2.2.4\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/5e/10/aa32dad071ce52b5502266b5c659451cfd6ffcbf14e6c8c4f16c0ff5aaab/Keras-2.2.4-py2.py3-none-any.whl (312kB)\n",
            "\r\u001b[K     |█                               | 10kB 22.3MB/s eta 0:00:01\r\u001b[K     |██                              | 20kB 28.9MB/s eta 0:00:01\r\u001b[K     |███▏                            | 30kB 30.0MB/s eta 0:00:01\r\u001b[K     |████▏                           | 40kB 32.0MB/s eta 0:00:01\r\u001b[K     |█████▎                          | 51kB 33.5MB/s eta 0:00:01\r\u001b[K     |██████▎                         | 61kB 36.1MB/s eta 0:00:01\r\u001b[K     |███████▍                        | 71kB 22.7MB/s eta 0:00:01\r\u001b[K     |████████▍                       | 81kB 23.9MB/s eta 0:00:01\r\u001b[K     |█████████▍                      | 92kB 21.7MB/s eta 0:00:01\r\u001b[K     |██████████▌                     | 102kB 22.5MB/s eta 0:00:01\r\u001b[K     |███████████▌                    | 112kB 22.5MB/s eta 0:00:01\r\u001b[K     |████████████▋                   | 122kB 22.5MB/s eta 0:00:01\r\u001b[K     |█████████████▋                  | 133kB 22.5MB/s eta 0:00:01\r\u001b[K     |██████████████▊                 | 143kB 22.5MB/s eta 0:00:01\r\u001b[K     |███████████████▊                | 153kB 22.5MB/s eta 0:00:01\r\u001b[K     |████████████████▊               | 163kB 22.5MB/s eta 0:00:01\r\u001b[K     |█████████████████▉              | 174kB 22.5MB/s eta 0:00:01\r\u001b[K     |██████████████████▉             | 184kB 22.5MB/s eta 0:00:01\r\u001b[K     |████████████████████            | 194kB 22.5MB/s eta 0:00:01\r\u001b[K     |█████████████████████           | 204kB 22.5MB/s eta 0:00:01\r\u001b[K     |██████████████████████          | 215kB 22.5MB/s eta 0:00:01\r\u001b[K     |███████████████████████         | 225kB 22.5MB/s eta 0:00:01\r\u001b[K     |████████████████████████▏       | 235kB 22.5MB/s eta 0:00:01\r\u001b[K     |█████████████████████████▏      | 245kB 22.5MB/s eta 0:00:01\r\u001b[K     |██████████████████████████▏     | 256kB 22.5MB/s eta 0:00:01\r\u001b[K     |███████████████████████████▎    | 266kB 22.5MB/s eta 0:00:01\r\u001b[K     |████████████████████████████▎   | 276kB 22.5MB/s eta 0:00:01\r\u001b[K     |█████████████████████████████▍  | 286kB 22.5MB/s eta 0:00:01\r\u001b[K     |██████████████████████████████▍ | 296kB 22.5MB/s eta 0:00:01\r\u001b[K     |███████████████████████████████▌| 307kB 22.5MB/s eta 0:00:01\r\u001b[K     |████████████████████████████████| 317kB 22.5MB/s \n",
            "\u001b[?25hRequirement already satisfied: h5py in /usr/local/lib/python3.7/dist-packages (from Keras==2.2.4) (2.10.0)\n",
            "Requirement already satisfied: keras-applications>=1.0.6 in /usr/local/lib/python3.7/dist-packages (from Keras==2.2.4) (1.0.8)\n",
            "Requirement already satisfied: keras-preprocessing>=1.0.5 in /usr/local/lib/python3.7/dist-packages (from Keras==2.2.4) (1.1.2)\n",
            "Requirement already satisfied: pyyaml in /usr/local/lib/python3.7/dist-packages (from Keras==2.2.4) (3.13)\n",
            "Requirement already satisfied: scipy>=0.14 in /usr/local/lib/python3.7/dist-packages (from Keras==2.2.4) (1.4.1)\n",
            "Requirement already satisfied: numpy>=1.9.1 in /usr/local/lib/python3.7/dist-packages (from Keras==2.2.4) (1.19.5)\n",
            "Requirement already satisfied: six>=1.9.0 in /usr/local/lib/python3.7/dist-packages (from Keras==2.2.4) (1.15.0)\n",
            "Installing collected packages: Keras\n",
            "  Found existing installation: Keras 2.4.3\n",
            "    Uninstalling Keras-2.4.3:\n",
            "      Successfully uninstalled Keras-2.4.3\n",
            "Successfully installed Keras-2.2.4\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.colab-display-data+json": {
              "pip_warning": {
                "packages": [
                  "keras"
                ]
              }
            }
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: jupyter in /usr/local/lib/python3.7/dist-packages (1.0.0)\n",
            "Requirement already satisfied: qtconsole in /usr/local/lib/python3.7/dist-packages (from jupyter) (5.0.3)\n",
            "Requirement already satisfied: ipywidgets in /usr/local/lib/python3.7/dist-packages (from jupyter) (7.6.3)\n",
            "Requirement already satisfied: ipykernel in /usr/local/lib/python3.7/dist-packages (from jupyter) (4.10.1)\n",
            "Requirement already satisfied: jupyter-console in /usr/local/lib/python3.7/dist-packages (from jupyter) (5.2.0)\n",
            "Requirement already satisfied: notebook in /usr/local/lib/python3.7/dist-packages (from jupyter) (5.3.1)\n",
            "Requirement already satisfied: nbconvert in /usr/local/lib/python3.7/dist-packages (from jupyter) (5.6.1)\n",
            "Requirement already satisfied: pyzmq>=17.1 in /usr/local/lib/python3.7/dist-packages (from qtconsole->jupyter) (22.0.3)\n",
            "Requirement already satisfied: traitlets in /usr/local/lib/python3.7/dist-packages (from qtconsole->jupyter) (5.0.5)\n",
            "Requirement already satisfied: jupyter-client>=4.1 in /usr/local/lib/python3.7/dist-packages (from qtconsole->jupyter) (5.3.5)\n",
            "Requirement already satisfied: qtpy in /usr/local/lib/python3.7/dist-packages (from qtconsole->jupyter) (1.9.0)\n",
            "Requirement already satisfied: pygments in /usr/local/lib/python3.7/dist-packages (from qtconsole->jupyter) (2.6.1)\n",
            "Requirement already satisfied: ipython-genutils in /usr/local/lib/python3.7/dist-packages (from qtconsole->jupyter) (0.2.0)\n",
            "Requirement already satisfied: jupyter-core in /usr/local/lib/python3.7/dist-packages (from qtconsole->jupyter) (4.7.1)\n",
            "Requirement already satisfied: jupyterlab-widgets>=1.0.0; python_version >= \"3.6\" in /usr/local/lib/python3.7/dist-packages (from ipywidgets->jupyter) (1.0.0)\n",
            "Requirement already satisfied: nbformat>=4.2.0 in /usr/local/lib/python3.7/dist-packages (from ipywidgets->jupyter) (5.1.3)\n",
            "Requirement already satisfied: widgetsnbextension~=3.5.0 in /usr/local/lib/python3.7/dist-packages (from ipywidgets->jupyter) (3.5.1)\n",
            "Requirement already satisfied: ipython>=4.0.0; python_version >= \"3.3\" in /usr/local/lib/python3.7/dist-packages (from ipywidgets->jupyter) (5.5.0)\n",
            "Requirement already satisfied: tornado>=4.0 in /usr/local/lib/python3.7/dist-packages (from ipykernel->jupyter) (5.1.1)\n",
            "Requirement already satisfied: prompt-toolkit<2.0.0,>=1.0.0 in /usr/local/lib/python3.7/dist-packages (from jupyter-console->jupyter) (1.0.18)\n",
            "Requirement already satisfied: terminado>=0.8.1 in /usr/local/lib/python3.7/dist-packages (from notebook->jupyter) (0.9.4)\n",
            "Requirement already satisfied: Send2Trash in /usr/local/lib/python3.7/dist-packages (from notebook->jupyter) (1.5.0)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.7/dist-packages (from notebook->jupyter) (2.11.3)\n",
            "Requirement already satisfied: bleach in /usr/local/lib/python3.7/dist-packages (from nbconvert->jupyter) (3.3.0)\n",
            "Requirement already satisfied: testpath in /usr/local/lib/python3.7/dist-packages (from nbconvert->jupyter) (0.4.4)\n",
            "Requirement already satisfied: defusedxml in /usr/local/lib/python3.7/dist-packages (from nbconvert->jupyter) (0.7.1)\n",
            "Requirement already satisfied: mistune<2,>=0.8.1 in /usr/local/lib/python3.7/dist-packages (from nbconvert->jupyter) (0.8.4)\n",
            "Requirement already satisfied: entrypoints>=0.2.2 in /usr/local/lib/python3.7/dist-packages (from nbconvert->jupyter) (0.3)\n",
            "Requirement already satisfied: pandocfilters>=1.4.1 in /usr/local/lib/python3.7/dist-packages (from nbconvert->jupyter) (1.4.3)\n",
            "Requirement already satisfied: python-dateutil>=2.1 in /usr/local/lib/python3.7/dist-packages (from jupyter-client>=4.1->qtconsole->jupyter) (2.8.1)\n",
            "Requirement already satisfied: jsonschema!=2.5.0,>=2.4 in /usr/local/lib/python3.7/dist-packages (from nbformat>=4.2.0->ipywidgets->jupyter) (2.6.0)\n",
            "Requirement already satisfied: pickleshare in /usr/local/lib/python3.7/dist-packages (from ipython>=4.0.0; python_version >= \"3.3\"->ipywidgets->jupyter) (0.7.5)\n",
            "Requirement already satisfied: setuptools>=18.5 in /usr/local/lib/python3.7/dist-packages (from ipython>=4.0.0; python_version >= \"3.3\"->ipywidgets->jupyter) (56.0.0)\n",
            "Requirement already satisfied: simplegeneric>0.8 in /usr/local/lib/python3.7/dist-packages (from ipython>=4.0.0; python_version >= \"3.3\"->ipywidgets->jupyter) (0.8.1)\n",
            "Requirement already satisfied: decorator in /usr/local/lib/python3.7/dist-packages (from ipython>=4.0.0; python_version >= \"3.3\"->ipywidgets->jupyter) (4.4.2)\n",
            "Requirement already satisfied: pexpect; sys_platform != \"win32\" in /usr/local/lib/python3.7/dist-packages (from ipython>=4.0.0; python_version >= \"3.3\"->ipywidgets->jupyter) (4.8.0)\n",
            "Requirement already satisfied: six>=1.9.0 in /usr/local/lib/python3.7/dist-packages (from prompt-toolkit<2.0.0,>=1.0.0->jupyter-console->jupyter) (1.15.0)\n",
            "Requirement already satisfied: wcwidth in /usr/local/lib/python3.7/dist-packages (from prompt-toolkit<2.0.0,>=1.0.0->jupyter-console->jupyter) (0.2.5)\n",
            "Requirement already satisfied: ptyprocess; os_name != \"nt\" in /usr/local/lib/python3.7/dist-packages (from terminado>=0.8.1->notebook->jupyter) (0.7.0)\n",
            "Requirement already satisfied: MarkupSafe>=0.23 in /usr/local/lib/python3.7/dist-packages (from jinja2->notebook->jupyter) (1.1.1)\n",
            "Requirement already satisfied: webencodings in /usr/local/lib/python3.7/dist-packages (from bleach->nbconvert->jupyter) (0.5.1)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.7/dist-packages (from bleach->nbconvert->jupyter) (20.9)\n",
            "Requirement already satisfied: pyparsing>=2.0.2 in /usr/local/lib/python3.7/dist-packages (from packaging->bleach->nbconvert->jupyter) (2.4.7)\n",
            "Requirement already satisfied: torch in /usr/local/lib/python3.7/dist-packages (1.8.1+cu101)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.7/dist-packages (from torch) (1.19.5)\n",
            "Requirement already satisfied: typing-extensions in /usr/local/lib/python3.7/dist-packages (from torch) (3.7.4.3)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LlJbCVRBJjN7"
      },
      "source": [
        "from __future__ import division\n",
        "\n",
        "from PIL import Image\n",
        "import numpy as np\n",
        "import gym\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "\n",
        "from keras.models import Sequential\n",
        "from keras.layers import Dense, Activation, Flatten, Convolution2D, Permute\n",
        "from keras.optimizers import Adam\n",
        "import keras.backend as K\n",
        "\n",
        "from rl.agents.dqn import DQNAgent\n",
        "from rl.policy import LinearAnnealedPolicy, BoltzmannQPolicy, EpsGreedyQPolicy\n",
        "from rl.memory import SequentialMemory\n",
        "from rl.core import Processor\n",
        "from rl.callbacks import FileLogger, ModelIntervalCheckpoint"
      ],
      "execution_count": 26,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MMjVEfFiJjN7"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ex5qFEZg_sLL"
      },
      "source": [
        "### Configuración del ambiente"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cyKV6wW9JjN7"
      },
      "source": [
        "INPUT_SHAPE = (84, 84) # tamaño de las imágenes\n",
        "WINDOW_LENGTH = 4\n",
        "\n",
        "env_name = 'SpaceInvaders-v0'\n",
        "env = gym.make(env_name)\n",
        "\n",
        "np.random.seed(42)\n",
        "env.seed(42)\n",
        "nb_actions = env.action_space.n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5xxeIFDZJjN7"
      },
      "source": [
        "class AtariProcessor(Processor):\n",
        "    def process_observation(self, observation):\n",
        "        assert observation.ndim == 3  # (height, width, channel)\n",
        "        img = Image.fromarray(observation)\n",
        "        img = img.resize(INPUT_SHAPE).convert('L')\n",
        "        processed_observation = np.array(img)\n",
        "        assert processed_observation.shape == INPUT_SHAPE\n",
        "        return processed_observation.astype('uint8')\n",
        "\n",
        "    def process_state_batch(self, batch):\n",
        "        processed_batch = batch.astype('float32') / 255.\n",
        "        return processed_batch\n",
        "\n",
        "    def process_reward(self, reward):\n",
        "        return np.clip(reward, -1., 1.)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MM8Ctnk5JjN8"
      },
      "source": [
        "### Implementación de la red neuronal"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "f5LshIuSJjN8",
        "outputId": "2943449d-2628-49fc-b4f2-0fb345d2d79f"
      },
      "source": [
        "input_shape = (WINDOW_LENGTH,) + INPUT_SHAPE\n",
        "model = Sequential()\n",
        "# model.add(Permute((2, 3, 1), input_shape=input_shape))\n",
        "\n",
        "# TODO\n",
        "\n",
        "if K.image_dim_ordering() == 'tf':\n",
        "    # (width, height, channels)\n",
        "    model.add(Permute((2, 3, 1), input_shape=input_shape))\n",
        "elif K.image_dim_ordering() == 'th':\n",
        "    # (channels, width, height)\n",
        "    model.add(Permute((1, 2, 3), input_shape=input_shape))\n",
        "else:\n",
        "    raise RuntimeError('Unknown image_dim_ordering.')\n",
        "\n",
        "model.add(Convolution2D(32, (8, 8), strides=(4, 4)))\n",
        "model.add(Activation('relu'))\n",
        "model.add(Convolution2D(64, (4, 4), strides=(2, 2)))\n",
        "model.add(Activation('relu'))\n",
        "model.add(Convolution2D(64, (3, 3), strides=(1, 1)))\n",
        "model.add(Activation('relu'))\n",
        "model.add(Flatten())\n",
        "model.add(Dense(512))\n",
        "model.add(Activation('relu'))\n",
        "model.add(Dense(nb_actions))\n",
        "model.add(Activation('linear'))\n",
        "# print(model.summary())"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "WARNING:tensorflow:From /usr/local/lib/python3.7/dist-packages/tensorflow/python/framework/op_def_library.py:263: colocate_with (from tensorflow.python.framework.ops) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Colocations handled automatically by placer.\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "permute_1 (Permute)          (None, 84, 84, 4)         0         \n",
            "_________________________________________________________________\n",
            "conv2d_1 (Conv2D)            (None, 20, 20, 32)        8224      \n",
            "_________________________________________________________________\n",
            "activation_1 (Activation)    (None, 20, 20, 32)        0         \n",
            "_________________________________________________________________\n",
            "conv2d_2 (Conv2D)            (None, 9, 9, 64)          32832     \n",
            "_________________________________________________________________\n",
            "activation_2 (Activation)    (None, 9, 9, 64)          0         \n",
            "_________________________________________________________________\n",
            "conv2d_3 (Conv2D)            (None, 7, 7, 64)          36928     \n",
            "_________________________________________________________________\n",
            "activation_3 (Activation)    (None, 7, 7, 64)          0         \n",
            "_________________________________________________________________\n",
            "flatten_1 (Flatten)          (None, 3136)              0         \n",
            "_________________________________________________________________\n",
            "dense_1 (Dense)              (None, 512)               1606144   \n",
            "_________________________________________________________________\n",
            "activation_4 (Activation)    (None, 512)               0         \n",
            "_________________________________________________________________\n",
            "dense_2 (Dense)              (None, 6)                 3078      \n",
            "_________________________________________________________________\n",
            "activation_5 (Activation)    (None, 6)                 0         \n",
            "=================================================================\n",
            "Total params: 1,687,206\n",
            "Trainable params: 1,687,206\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n",
            "None\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7BeUfgBOJjN8"
      },
      "source": [
        "#### Selección de hiperparámetros para la solución DQN"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OCSVAXV2JjN8"
      },
      "source": [
        "# TODO - Select the parameters for the memory\n",
        "memory = SequentialMemory(limit=50_000, window_length=WINDOW_LENGTH)\n",
        "processor = AtariProcessor()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "p6eY66lnJjN9"
      },
      "source": [
        "# TODO - Select the parameters for the policy\n",
        "policy = LinearAnnealedPolicy(EpsGreedyQPolicy(), \n",
        "                              attr='eps', \n",
        "                              value_max=1, \n",
        "                              value_min=0.1, \n",
        "                              value_test=0.5, \n",
        "                              nb_steps=500_000)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KyOrNHLIJjN9"
      },
      "source": [
        "# TODO - Select the parameters for the Agent and the Optimizer\n",
        "dqn = DQNAgent(model, \n",
        "               nb_actions=nb_actions, \n",
        "               memory=memory,\n",
        "               processor=processor, target_model_update=10000,\n",
        "               train_interval=20)\n",
        "dqn.compile(Adam(), metrics=['mae'])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7j4mb44I_4Ox"
      },
      "source": [
        "#### Entrenamiento del modelo"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "OvhO34hMJjN9",
        "outputId": "0980ce74-731c-4000-849a-8a36f11072b8"
      },
      "source": [
        "# Training part\n",
        "weights_filename = 'dqn_{}_weights.h5f'.format(env_name)\n",
        "checkpoint_weights_filename = 'dqn_' + env_name + '_weights_{step}.h5f'\n",
        "log_filename = 'dqn_{}_log.json'.format(env_name)\n",
        "callbacks = [ModelIntervalCheckpoint(checkpoint_weights_filename, interval=200_000)]\n",
        "callbacks += [FileLogger(log_filename, interval=100)]\n",
        "\n",
        "# TODO - Select the parameters for the method \"fit\"\n",
        "dqn.fit(env, callbacks=callbacks, nb_steps=500_000, log_interval=10000, visualize=False)\n",
        "\n",
        "dqn.save_weights(weights_filename, overwrite=True)\n",
        "dqn.save_weights(BASE_FOLDER+weights_filename, overwrite=True)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Training for 500000 steps ...\n",
            "Interval 1 (0 steps performed)\n",
            "10000/10000 [==============================] - 103s 10ms/step - reward: 0.0170\n",
            "11 episodes - episode_reward: 14.273 [6.000, 25.000] - loss: 0.009 - mean_absolute_error: 0.047 - mean_q: 0.087 - ale.lives: 1.926\n",
            "\n",
            "Interval 2 (10000 steps performed)\n",
            "10000/10000 [==============================] - 108s 11ms/step - reward: 0.0172\n",
            "12 episodes - episode_reward: 14.667 [7.000, 26.000] - loss: 0.009 - mean_absolute_error: 0.065 - mean_q: 0.105 - ale.lives: 1.824\n",
            "\n",
            "Interval 3 (20000 steps performed)\n",
            "10000/10000 [==============================] - 111s 11ms/step - reward: 0.0189\n",
            "12 episodes - episode_reward: 16.500 [7.000, 25.000] - loss: 0.008 - mean_absolute_error: 0.081 - mean_q: 0.119 - ale.lives: 1.964\n",
            "\n",
            "Interval 4 (30000 steps performed)\n",
            "10000/10000 [==============================] - 113s 11ms/step - reward: 0.0166\n",
            "12 episodes - episode_reward: 13.833 [5.000, 25.000] - loss: 0.010 - mean_absolute_error: 0.104 - mean_q: 0.147 - ale.lives: 2.052\n",
            "\n",
            "Interval 5 (40000 steps performed)\n",
            "10000/10000 [==============================] - 113s 11ms/step - reward: 0.0154\n",
            "13 episodes - episode_reward: 11.462 [5.000, 19.000] - loss: 0.009 - mean_absolute_error: 0.116 - mean_q: 0.158 - ale.lives: 2.173\n",
            "\n",
            "Interval 6 (50000 steps performed)\n",
            "10000/10000 [==============================] - 110s 11ms/step - reward: 0.0154\n",
            "10 episodes - episode_reward: 15.000 [5.000, 25.000] - loss: 0.009 - mean_absolute_error: 0.136 - mean_q: 0.182 - ale.lives: 1.924\n",
            "\n",
            "Interval 7 (60000 steps performed)\n",
            "10000/10000 [==============================] - 108s 11ms/step - reward: 0.0126\n",
            "15 episodes - episode_reward: 8.733 [4.000, 19.000] - loss: 0.008 - mean_absolute_error: 0.133 - mean_q: 0.176 - ale.lives: 2.028\n",
            "\n",
            "Interval 8 (70000 steps performed)\n",
            "10000/10000 [==============================] - 107s 11ms/step - reward: 0.0175\n",
            "12 episodes - episode_reward: 14.917 [7.000, 27.000] - loss: 0.008 - mean_absolute_error: 0.140 - mean_q: 0.183 - ale.lives: 2.106\n",
            "\n",
            "Interval 9 (80000 steps performed)\n",
            "10000/10000 [==============================] - 107s 11ms/step - reward: 0.0168\n",
            "12 episodes - episode_reward: 13.750 [2.000, 21.000] - loss: 0.008 - mean_absolute_error: 0.142 - mean_q: 0.184 - ale.lives: 1.976\n",
            "\n",
            "Interval 10 (90000 steps performed)\n",
            "10000/10000 [==============================] - 106s 11ms/step - reward: 0.0184\n",
            "12 episodes - episode_reward: 14.750 [5.000, 25.000] - loss: 0.009 - mean_absolute_error: 0.164 - mean_q: 0.212 - ale.lives: 1.904\n",
            "\n",
            "Interval 11 (100000 steps performed)\n",
            "10000/10000 [==============================] - 106s 11ms/step - reward: 0.0137\n",
            "13 episodes - episode_reward: 11.077 [5.000, 19.000] - loss: 0.008 - mean_absolute_error: 0.178 - mean_q: 0.228 - ale.lives: 2.015\n",
            "\n",
            "Interval 12 (110000 steps performed)\n",
            "10000/10000 [==============================] - 105s 10ms/step - reward: 0.0133\n",
            "14 episodes - episode_reward: 9.571 [4.000, 19.000] - loss: 0.008 - mean_absolute_error: 0.199 - mean_q: 0.252 - ale.lives: 1.970\n",
            "\n",
            "Interval 13 (120000 steps performed)\n",
            "10000/10000 [==============================] - 105s 11ms/step - reward: 0.0147\n",
            "13 episodes - episode_reward: 10.923 [4.000, 21.000] - loss: 0.009 - mean_absolute_error: 0.208 - mean_q: 0.265 - ale.lives: 1.998\n",
            "\n",
            "Interval 14 (130000 steps performed)\n",
            "10000/10000 [==============================] - 105s 10ms/step - reward: 0.0182\n",
            "12 episodes - episode_reward: 14.417 [7.000, 21.000] - loss: 0.008 - mean_absolute_error: 0.216 - mean_q: 0.276 - ale.lives: 2.046\n",
            "\n",
            "Interval 15 (140000 steps performed)\n",
            "10000/10000 [==============================] - 106s 11ms/step - reward: 0.0157\n",
            "14 episodes - episode_reward: 11.714 [3.000, 20.000] - loss: 0.008 - mean_absolute_error: 0.223 - mean_q: 0.280 - ale.lives: 2.208\n",
            "\n",
            "Interval 16 (150000 steps performed)\n",
            "10000/10000 [==============================] - 108s 11ms/step - reward: 0.0157\n",
            "12 episodes - episode_reward: 12.917 [6.000, 23.000] - loss: 0.008 - mean_absolute_error: 0.249 - mean_q: 0.314 - ale.lives: 2.080\n",
            "\n",
            "Interval 17 (160000 steps performed)\n",
            "10000/10000 [==============================] - 106s 11ms/step - reward: 0.0141\n",
            "13 episodes - episode_reward: 11.308 [3.000, 21.000] - loss: 0.010 - mean_absolute_error: 0.261 - mean_q: 0.327 - ale.lives: 1.981\n",
            "\n",
            "Interval 18 (170000 steps performed)\n",
            "10000/10000 [==============================] - 105s 11ms/step - reward: 0.0170\n",
            "12 episodes - episode_reward: 13.583 [6.000, 23.000] - loss: 0.009 - mean_absolute_error: 0.268 - mean_q: 0.335 - ale.lives: 2.100\n",
            "\n",
            "Interval 19 (180000 steps performed)\n",
            "10000/10000 [==============================] - 106s 11ms/step - reward: 0.0145\n",
            "16 episodes - episode_reward: 9.375 [4.000, 19.000] - loss: 0.008 - mean_absolute_error: 0.281 - mean_q: 0.351 - ale.lives: 2.018\n",
            "\n",
            "Interval 20 (190000 steps performed)\n",
            "10000/10000 [==============================] - 106s 11ms/step - reward: 0.0155\n",
            "16 episodes - episode_reward: 10.000 [5.000, 15.000] - loss: 0.009 - mean_absolute_error: 0.303 - mean_q: 0.376 - ale.lives: 2.082\n",
            "\n",
            "Interval 21 (200000 steps performed)\n",
            "10000/10000 [==============================] - 105s 10ms/step - reward: 0.0162\n",
            "15 episodes - episode_reward: 10.800 [4.000, 23.000] - loss: 0.008 - mean_absolute_error: 0.317 - mean_q: 0.395 - ale.lives: 1.987\n",
            "\n",
            "Interval 22 (210000 steps performed)\n",
            "10000/10000 [==============================] - 105s 11ms/step - reward: 0.0149\n",
            "14 episodes - episode_reward: 9.286 [5.000, 20.000] - loss: 0.008 - mean_absolute_error: 0.316 - mean_q: 0.392 - ale.lives: 2.117\n",
            "\n",
            "Interval 23 (220000 steps performed)\n",
            "10000/10000 [==============================] - 106s 11ms/step - reward: 0.0180\n",
            "13 episodes - episode_reward: 15.462 [8.000, 23.000] - loss: 0.008 - mean_absolute_error: 0.336 - mean_q: 0.417 - ale.lives: 1.906\n",
            "\n",
            "Interval 24 (230000 steps performed)\n",
            "10000/10000 [==============================] - 106s 11ms/step - reward: 0.0171\n",
            "13 episodes - episode_reward: 12.000 [5.000, 19.000] - loss: 0.009 - mean_absolute_error: 0.356 - mean_q: 0.440 - ale.lives: 2.009\n",
            "\n",
            "Interval 25 (240000 steps performed)\n",
            "10000/10000 [==============================] - 106s 11ms/step - reward: 0.0167\n",
            "14 episodes - episode_reward: 12.643 [2.000, 24.000] - loss: 0.010 - mean_absolute_error: 0.375 - mean_q: 0.466 - ale.lives: 2.060\n",
            "\n",
            "Interval 26 (250000 steps performed)\n",
            "10000/10000 [==============================] - 106s 11ms/step - reward: 0.0162\n",
            "14 episodes - episode_reward: 11.071 [5.000, 20.000] - loss: 0.010 - mean_absolute_error: 0.391 - mean_q: 0.481 - ale.lives: 2.117\n",
            "\n",
            "Interval 27 (260000 steps performed)\n",
            "10000/10000 [==============================] - 105s 11ms/step - reward: 0.0179\n",
            "15 episodes - episode_reward: 12.733 [4.000, 22.000] - loss: 0.010 - mean_absolute_error: 0.400 - mean_q: 0.494 - ale.lives: 2.078\n",
            "\n",
            "Interval 28 (270000 steps performed)\n",
            "10000/10000 [==============================] - 105s 11ms/step - reward: 0.0179\n",
            "15 episodes - episode_reward: 11.067 [3.000, 23.000] - loss: 0.011 - mean_absolute_error: 0.418 - mean_q: 0.516 - ale.lives: 2.085\n",
            "\n",
            "Interval 29 (280000 steps performed)\n",
            "10000/10000 [==============================] - 105s 11ms/step - reward: 0.0144\n",
            "16 episodes - episode_reward: 9.625 [1.000, 20.000] - loss: 0.009 - mean_absolute_error: 0.432 - mean_q: 0.531 - ale.lives: 2.026\n",
            "\n",
            "Interval 30 (290000 steps performed)\n",
            "10000/10000 [==============================] - 104s 10ms/step - reward: 0.0156\n",
            "14 episodes - episode_reward: 10.500 [2.000, 17.000] - loss: 0.010 - mean_absolute_error: 0.436 - mean_q: 0.536 - ale.lives: 1.994\n",
            "\n",
            "Interval 31 (300000 steps performed)\n",
            "10000/10000 [==============================] - 105s 10ms/step - reward: 0.0155\n",
            "13 episodes - episode_reward: 12.615 [3.000, 28.000] - loss: 0.010 - mean_absolute_error: 0.444 - mean_q: 0.545 - ale.lives: 2.014\n",
            "\n",
            "Interval 32 (310000 steps performed)\n",
            "10000/10000 [==============================] - 105s 11ms/step - reward: 0.0150\n",
            "12 episodes - episode_reward: 11.917 [3.000, 30.000] - loss: 0.010 - mean_absolute_error: 0.452 - mean_q: 0.554 - ale.lives: 2.092\n",
            "\n",
            "Interval 33 (320000 steps performed)\n",
            "10000/10000 [==============================] - 105s 10ms/step - reward: 0.0150\n",
            "14 episodes - episode_reward: 11.000 [5.000, 23.000] - loss: 0.008 - mean_absolute_error: 0.462 - mean_q: 0.568 - ale.lives: 2.077\n",
            "\n",
            "Interval 34 (330000 steps performed)\n",
            "10000/10000 [==============================] - 104s 10ms/step - reward: 0.0162\n",
            "13 episodes - episode_reward: 12.308 [6.000, 21.000] - loss: 0.009 - mean_absolute_error: 0.459 - mean_q: 0.564 - ale.lives: 1.953\n",
            "\n",
            "Interval 35 (340000 steps performed)\n",
            "10000/10000 [==============================] - 103s 10ms/step - reward: 0.0161\n",
            "14 episodes - episode_reward: 12.000 [4.000, 28.000] - loss: 0.010 - mean_absolute_error: 0.466 - mean_q: 0.573 - ale.lives: 2.067\n",
            "\n",
            "Interval 36 (350000 steps performed)\n",
            "10000/10000 [==============================] - 103s 10ms/step - reward: 0.0163\n",
            "12 episodes - episode_reward: 12.167 [4.000, 20.000] - loss: 0.009 - mean_absolute_error: 0.469 - mean_q: 0.576 - ale.lives: 2.098\n",
            "\n",
            "Interval 37 (360000 steps performed)\n",
            "10000/10000 [==============================] - 102s 10ms/step - reward: 0.0157\n",
            "14 episodes - episode_reward: 12.143 [4.000, 21.000] - loss: 0.010 - mean_absolute_error: 0.484 - mean_q: 0.595 - ale.lives: 2.065\n",
            "\n",
            "Interval 38 (370000 steps performed)\n",
            "10000/10000 [==============================] - 102s 10ms/step - reward: 0.0154\n",
            "14 episodes - episode_reward: 11.286 [3.000, 29.000] - loss: 0.009 - mean_absolute_error: 0.497 - mean_q: 0.613 - ale.lives: 2.025\n",
            "\n",
            "Interval 39 (380000 steps performed)\n",
            "10000/10000 [==============================] - 102s 10ms/step - reward: 0.0204\n",
            "12 episodes - episode_reward: 16.667 [5.000, 29.000] - loss: 0.010 - mean_absolute_error: 0.523 - mean_q: 0.642 - ale.lives: 2.150\n",
            "\n",
            "Interval 40 (390000 steps performed)\n",
            "10000/10000 [==============================] - 102s 10ms/step - reward: 0.0174\n",
            "13 episodes - episode_reward: 13.538 [5.000, 25.000] - loss: 0.010 - mean_absolute_error: 0.535 - mean_q: 0.656 - ale.lives: 2.135\n",
            "\n",
            "Interval 41 (400000 steps performed)\n",
            "10000/10000 [==============================] - 102s 10ms/step - reward: 0.0160\n",
            "11 episodes - episode_reward: 14.818 [4.000, 26.000] - loss: 0.010 - mean_absolute_error: 0.539 - mean_q: 0.660 - ale.lives: 2.045\n",
            "\n",
            "Interval 42 (410000 steps performed)\n",
            "10000/10000 [==============================] - 103s 10ms/step - reward: 0.0174\n",
            "13 episodes - episode_reward: 12.692 [6.000, 25.000] - loss: 0.010 - mean_absolute_error: 0.559 - mean_q: 0.683 - ale.lives: 2.062\n",
            "\n",
            "Interval 43 (420000 steps performed)\n",
            "10000/10000 [==============================] - 102s 10ms/step - reward: 0.0178\n",
            "13 episodes - episode_reward: 13.846 [6.000, 23.000] - loss: 0.011 - mean_absolute_error: 0.583 - mean_q: 0.714 - ale.lives: 1.962\n",
            "\n",
            "Interval 44 (430000 steps performed)\n",
            "10000/10000 [==============================] - 103s 10ms/step - reward: 0.0169\n",
            "14 episodes - episode_reward: 12.357 [4.000, 26.000] - loss: 0.012 - mean_absolute_error: 0.600 - mean_q: 0.735 - ale.lives: 1.958\n",
            "\n",
            "Interval 45 (440000 steps performed)\n",
            "10000/10000 [==============================] - 103s 10ms/step - reward: 0.0190\n",
            "13 episodes - episode_reward: 14.231 [4.000, 24.000] - loss: 0.011 - mean_absolute_error: 0.614 - mean_q: 0.749 - ale.lives: 2.092\n",
            "\n",
            "Interval 46 (450000 steps performed)\n",
            "10000/10000 [==============================] - 102s 10ms/step - reward: 0.0170\n",
            "10 episodes - episode_reward: 17.500 [9.000, 30.000] - loss: 0.011 - mean_absolute_error: 0.625 - mean_q: 0.762 - ale.lives: 2.000\n",
            "\n",
            "Interval 47 (460000 steps performed)\n",
            "10000/10000 [==============================] - 102s 10ms/step - reward: 0.0151\n",
            "13 episodes - episode_reward: 10.615 [4.000, 19.000] - loss: 0.011 - mean_absolute_error: 0.633 - mean_q: 0.771 - ale.lives: 1.975\n",
            "\n",
            "Interval 48 (470000 steps performed)\n",
            "10000/10000 [==============================] - 101s 10ms/step - reward: 0.0175\n",
            "13 episodes - episode_reward: 13.077 [2.000, 21.000] - loss: 0.011 - mean_absolute_error: 0.649 - mean_q: 0.793 - ale.lives: 1.867\n",
            "\n",
            "Interval 49 (480000 steps performed)\n",
            "10000/10000 [==============================] - 102s 10ms/step - reward: 0.0175\n",
            "13 episodes - episode_reward: 14.385 [5.000, 33.000] - loss: 0.011 - mean_absolute_error: 0.675 - mean_q: 0.825 - ale.lives: 2.145\n",
            "\n",
            "Interval 50 (490000 steps performed)\n",
            "10000/10000 [==============================] - 103s 10ms/step - reward: 0.0158\n",
            "done, took 5257.787 seconds\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0oIURgSr_84-"
      },
      "source": [
        "#### Testeo del modelo"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "LU4lSS3XJjN9",
        "outputId": "0cd831e0-8425-4e58-bce0-f0a6db0fd3c1"
      },
      "source": [
        "# Testing part to calculate the mean reward\n",
        "weights_filename = 'dqn_{}_weights.h5f'.format(env_name)\n",
        "dqn.load_weights(BASE_FOLDER+weights_filename)\n",
        "dqn.test(env, nb_episodes=10, visualize=False)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Testing for 10 episodes ...\n",
            "Episode 1: reward: 14.000, steps: 743\n",
            "Episode 2: reward: 7.000, steps: 389\n",
            "Episode 3: reward: 24.000, steps: 1201\n",
            "Episode 4: reward: 14.000, steps: 823\n",
            "Episode 5: reward: 16.000, steps: 1584\n",
            "Episode 6: reward: 15.000, steps: 952\n",
            "Episode 7: reward: 5.000, steps: 361\n",
            "Episode 8: reward: 13.000, steps: 718\n",
            "Episode 9: reward: 22.000, steps: 1134\n",
            "Episode 10: reward: 22.000, steps: 1046\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<keras.callbacks.History at 0x7fcdbe054610>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 26
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "o4eQuVtyACjc"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "k6MEdLUAACOY"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GUXwASjpJjN-"
      },
      "source": [
        "## Justificación de los parámetros seleccionados y de los resultados obtenidos"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lDxpt0LhJjN-"
      },
      "source": [
        "#### los principales hiperparámetros que se usaron fueron:\n",
        "\n",
        "* Memory limit: este parámetro se configuró en 50,000. Originalmente intenté con valores de 1 millón pero los cálculos eran muy lentos pues una memoria tan grande hace más lento el proceso del Q-learning. Entonces lo reduje hasta 50,000, punto en el cual observaba que los resultados eran decentes y las simulaciones tardaban mucho menos (al rededor de 2 horas vs 10-15 horas cuando el parámetro era de 1 millón)\n",
        "\n",
        "* número de steps: Este parámetro fue configurado por medio de ensayo y error. Estaba buscando un número de steps que fuera lo suficientemente grande como para que el modelo fuera bueno, pero no tan grande como para que el modelo se demorara mucho y bajara su ajuste (en este caso la media de los rewards). \n",
        "Al configurarlo en 100.000 o más, notaba que los resultados empezaban a empeorar. Con 50.000 los resultados seguían siendo decentes y el tiempo de las simulaciones se redujo mucho."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "__QCNBgm-IPP"
      },
      "source": [
        "#### Resultados"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kwdopgLv8fDn"
      },
      "source": [
        "## Resultados obtenidos\n",
        "rewards = np.array([14, 7, 24, 14, 16, 15, 5, 13, 22, 22])\n",
        "steps_simul = np.array([743, 389,1201,823,1584,952,361,718,1134,1046])"
      ],
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 281
        },
        "id": "ogBvXFwg61rn",
        "outputId": "e898c0f3-1f23-4bbe-ae6c-fbbce706552f"
      },
      "source": [
        "plt.boxplot(rewards)\n",
        "\n",
        "plt.title(\"distribución de los rewards\")\n",
        "plt.show()"
      ],
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": [],
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "zb4IYhD8-bqA",
        "outputId": "4bcad8cf-24d2-45e9-c0dd-56198faa43ff"
      },
      "source": [
        "print(f\"Se obtuvo una media de rewards de {rewards.mean()}\")\n",
        "print(f\"Se obtuvo una desviación estándar de rewards de {rewards.std()}\")\n",
        "print(f\"Se obtuvo un reward mínimo de {rewards.min()}\")\n",
        "print(f\"Se obtuvo un reward máximo de {rewards.max()}\")"
      ],
      "execution_count": 24,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Se obtuvo una media de rewards de 15.2\n",
            "Se obtuvo una desviación estándar de rewards de 5.912698199637793\n",
            "Se obtuvo un reward mínimo de 5\n",
            "Se obtuvo un reward máximo de 24\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ugnSjFz3-ioi"
      },
      "source": [
        "Se obtuvo una media de 15.2 en los rewards con una desviación estándar de 5.91. \n",
        "\n",
        "La distribución es sesgada hacia la derecha, se observaron rewards entre 5 (mínimo) y 24 (máximo)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RG85i5AVVwUv"
      },
      "source": [
        ""
      ],
      "execution_count": 24,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yIqN96xZ-8gs"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}